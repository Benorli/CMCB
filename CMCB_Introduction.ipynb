{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational Modeling of Cognition and Behavior\n",
    "\n",
    "# Introduction\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Models and Theories in Science\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Data never speak for themselves, a model is unobservable, it requires understanding and explanation.\n",
    "\n",
    "2. Verbal theorizing can't replace quantitative analysis.\n",
    "\n",
    "3. There are always many models, we must select between them.\n",
    "\n",
    "4. Model selection requires both:\n",
    "    - Quantitave evaluation.\n",
    "    - Intellectual judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Quantitative Modeling in Cognition\n",
    "***\n",
    "\n",
    "### 1.2.1 Models and Data\n",
    "\n",
    "<br>\n",
    "\n",
    "Appropriate models can explain a great deal of variance, which is not intuitivly obvious when looking at the data.\n",
    "\n",
    "<br>\n",
    "\n",
    "Models fit into two broad catagories:\n",
    "\n",
    "1. Models that describe data\n",
    "\n",
    "2. Models that explain an underlying cognitive process\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.2.2 Data Description\n",
    "\n",
    "<br>\n",
    "\n",
    "Models are used to summarise and communicate data, a mean could be considered a model.\n",
    "\n",
    "Select a model which best represents the data, for example median or trimmed mean can be better when data has skew.\n",
    "\n",
    "Is learning a \"**Power Law**\" or exponential improvement? Improvements in new skills are large in the first trial, but then plateau. Best to try and define as a function. \n",
    "\n",
    "$$\n",
    "RT=N^{-\\beta}\n",
    "$$\n",
    "\n",
    "$RT$ - Time to perform task \n",
    "\n",
    "$N$ - Number of learning trials to date\n",
    "\n",
    "$\\beta$ - Learning rate\n",
    "\n",
    "<br>\n",
    "\n",
    "Following Palmeri (1997)'s data (see fig 1.5), Heathcote et al. (2000) suggested the following exponential function is a better fit:\n",
    "\n",
    "$$\n",
    "RT=e^{-\\alpha N}\n",
    "$$\n",
    "\n",
    "$\\alpha$ - Learning rate\n",
    "\n",
    "To fit the data, they also included an asymptote ($+A$) and a multiplier ($\\times B$) (see fig 1.5).\n",
    "\n",
    "However, both models are very similar, and in this case, the Power function has a slightly lower route mean-squared deviation (RMSD) than the exponential model.\n",
    "\n",
    "<br>\n",
    "\n",
    "Each model has different implications:\n",
    "\n",
    "- **Power function**: The rate of learning *decreases* with increasing practice.\n",
    "\n",
    "- **Exponential function**: The learning rate, relative to what remains to be learned remains constant with practice. Learning continues to enhance your knowledge by a constant fraction.\n",
    "\n",
    "<br>\n",
    "\n",
    "Heathcote et al. (2000) analysed a larger body of data and found the exponential model is a better description of skill acquisition. Resulting in a paradigm shift.\n",
    "\n",
    "1. Choice of model gives implications about the process\n",
    "\n",
    "2. Model choice must be done given strict quantitative criteria (Chapter 10)\n",
    "\n",
    "3. Heathcote et al.'s model selection considered individual subjects, not just averages across participants. Raising the issue: How best to apply a model for data with multiple participants? (Chapter 5)\n",
    "\n",
    "<br>\n",
    "\n",
    "Questions may also be asked regarding the rate of forgetting. Two conclusions from Wixted (2004):\n",
    "\n",
    "1. \"The degree of learning does not affect the rate of forgetting\".\n",
    "\n",
    "2. The rate of loss *decelerates* over time.\n",
    "\n",
    "<br>\n",
    "\n",
    "Follows the same pattern as before, requiring quantitative data to make a selection and the choice of model has psychological implications.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Normative Behaviour** : \"How people would behave if following the rules of logic or probability\n",
    "\n",
    "People tend to violate normative expectations, even in very simple examples.\n",
    "\n",
    "<br>\n",
    "\n",
    "Descriptive models contain no psychological conent, they simply describe the data. Some models do, such as \"process models\", which explain and underlying cognitive process\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1.2.2 Cognitive Process Models\n",
    "\n",
    "<br>\n",
    "\n",
    "**Generalized Context Model (GCM)**: Classfies stimuili. Stores every category example enocuntered during training in memory and reffers to those to catagerise test stimuli.\n",
    "\n",
    "<br>\n",
    "\n",
    "$i$: A particular test stimulus. <br>\n",
    "\n",
    "$j$: A particular example stimulus. <br>\n",
    "\n",
    "$I$, $J$: The number of elements in the respective sets <br>\n",
    "\n",
    "$\\mathfrak{I}$: the set of test stimuli <br>\n",
    "\n",
    "$\\mathfrak{J}$: the set of examples <br>\n",
    "\n",
    "$j = 1,2,...,J$ hence $j \\in \\mathfrak{J}$ \n",
    "\n",
    "<br>\n",
    "\n",
    "Note: Lowercase ($i,j$); specific set elements. Upercase ($I,J$); number of elements in a set.\n",
    "\n",
    "Test stimuli are compared to all stored examples, *similarity* beween $i$ and each $j$ is determined. GCM assumes similarity is proximity in perceptual space. Formally defined using the Pythagorean theorem.\n",
    "\n",
    "$$\n",
    "d_{ij} = \\sqrt{ \\left(\\sum_{k=1}^{K}(x_{ik} - x_{jk})^2 \\right)}\n",
    "$$\n",
    "\n",
    "$d_{ij}$: distance between $i$ and $j$ <br>\n",
    "$k$: dimension <br>\n",
    "$x_{ik}$: value of dimension $k$ for test item $i$ <br>\n",
    "$x_{ij}$: value of dimension $k$ for the stored example $j$ \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "The number of dimensions is arbitrary, 2 easy to demonstrate but original example used 4 dimensions to characterize cartoon faces (eye height, eye seperation, nose length, mough height). \n",
    "\n",
    "GCM postulates similarity is defined:\n",
    "\n",
    "$$\n",
    "s_{ij} = e^{-c \\cdot d_{ij}}\n",
    "$$\n",
    "\n",
    "$c$: a parameter <br>\n",
    "$s_{ij}$: similarity\n",
    "\n",
    "<br>\n",
    "\n",
    "Meaning similarity has an exponential relationship with the distance (see fig 1.7). The GCM can generalize to never before seen data. \n",
    "\n",
    "Using this method, all test stimuli can be given a similarity score to each memorized example. Now a decision can be made. Activiations can be summed seperatly accross examples from each catagory. The relative magnitude of the sums is as follows: Summed catagagory $A$ similarity, over the sum of catagory $A$ and $B$ similarity;\n",
    "\n",
    "$$\n",
    "P(R_{i} = A|i) = \\frac{\\left(\\sum_{j\\in A}^{} s_{ij} \\right)}{\\left(\\sum_{j\\in A}^{} s_{ij} \\right) + \\left(\\sum_{j\\in B}^{} s_{ij} \\right)}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$A,B$: Catagories <br>\n",
    "$P(R_{i} = A|i)$: The probability of classifying stimulus $i$ into catagory $A$. \n",
    "\n",
    "The choice of mathematics is derived from 'deeper' principles such as \"the universal law of generalization\" (Shepard 1987) and theoretical approaches developed by Luce (1963).\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.3 Potential Problems: Scope and Falsifiability\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "Theories should be falsifiable/testable; there are some possible outcomes which are not compatible with the theory's predictions. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Outcome space**: When the 'outcome space' of a theory is small, the likelihood of a match between predictions and data is much less. For this reason, when data matches predicitons of a theory with small 'outcome space', the support for the theory is stronger than if it had a larger outcome space. (Dunn, 2000, for more formalized view)<br>\n",
    "\n",
    "**Quality of data**: When all data fits within the prediction, it provides stronger support for a theory than when data varies outside of the predicted outcome space.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.4 Modeling as a “Cognitive Aid” for the Scientist\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "    \n",
    "A discussion of the importance of the replicability of studies.\n",
    "\n",
    "<br>\n",
    "\n",
    "Communication between researchers is leaky and incomplete. Ideas can morph over time and by degrees of seperation from the 'source'. The analogy chosen can have a significant impact on people's understanding of an idea. Sometimes only some features of a model are shared in peoples understanding. When a person believes they are testing a model, the originator of the theory may reject the test and believe their theory is predicts something different. \n",
    "\n",
    "<br>\n",
    "\n",
    "The specificity of computational models reduces the ambiguity in communication.\n",
    "\n",
    "<br>\n",
    "\n",
    "Computational models determine if intuitions of a theorized system match it's actual consequences. It clarifies theories. Implamentation exposes where decisions must be made about methods and mechanisms. \n",
    "\n",
    "<br>\n",
    "\n",
    "## 1.5 In Vivo Modeling: “Cognitive Aid” or “Cognitive Burden”?\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "Start simple. Get eperience with simple examples before moving to complex examples.\n",
    "\n",
    "Multinomial processing tree (MPT) models, have wide applications in cognitive research. They can be used on aggregate data or per individual. Often there is not enogh data on individuals so a heirachical structure, (e.g., Matzke et al., 2015; Smith and Batchelder, 2010) can allow for adjustments to be made to individuals (paramters drawn from a distrubution), while using aggregate data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
